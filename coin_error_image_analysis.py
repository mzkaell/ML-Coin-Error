# -*- coding: utf-8 -*-
"""Coin Error Image Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qDpb016SWJNEealhoBle-8e86UDGFBiR
"""



import os
import xml.etree.ElementTree as ET
import pandas as pd
import zipfile
from google.colab import files
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import models, layers
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)


from google.colab import drive
drive.mount('/content/drive')

zip_file_path = '/content/drive/My Drive/Double Die.zip'

# Create a directory for the unzipped contents if it doesn't exist
unzip_dir = '/content/Double Die'
os.makedirs(unzip_dir, exist_ok=True)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(unzip_dir)

# List the contents of the unzipped directory for debugging
print(os.listdir(unzip_dir))

# Set the data folder where your images and XML files are located
train_folder = '/content/Double Die/Train/'
val_folder = '/content/Double Die/Val/'


print(os.listdir(train_folder))
print(os.listdir(val_folder))

# Function to load images and labels from a specified folder
def load_data(folder):
    images = [f for f in os.listdir(folder) if f.endswith('.jpg')]
    labels = []

    for img in images:
        xml_file = img.replace('.jpg', '.xml')
        xml_path = os.path.join(folder, xml_file)
        if os.path.exists(xml_path):
            tree = ET.parse(xml_path)
            root = tree.getroot()

            # Extract the label (adjust based on your XML structure)
            for obj in root.findall('object'):
                label = obj.find('name').text
                label = label.lower().replace(" ", "")
                labels.append(label)
                print(f'Image: {img}, Label: {label}')  # Debugging output
        else:
            print(f'Missing XML for {img}')
            labels.append('singledie')  # Default label for missing XML files

    return images, labels

train_images, train_labels = load_data(train_folder)
train_data = {'filename': train_images, 'class': train_labels}
train_df = pd.DataFrame(train_data)

# Load validation data
val_images, val_labels = load_data(val_folder)
val_data = {'filename': val_images, 'class': val_labels}
val_df = pd.DataFrame(val_data)

print(train_df.shape)
print(val_df.shape)

# Print DataFrame structure for debugging
print(train_df.head())  # Check train DataFrame
print(val_df.head())    # Check validation DataFrame
print(train_df['class'].unique())
print(val_df['class'].unique())
print(train_df['class'].dtype)
print(val_df['class'].dtype)

print("Train DataFrame Filenames:")
print(train_df['filename'])  # Replace 'filename' with the actual column name
print("\nValidation DataFrame Filenames:")
print(val_df['filename'])     # Same here
print("Train Generator:", train_generator)
print("Validation Generator:", val_generator)
for data in val_generator:
      print(data)  # Check the output
      break  # Just to see one iteration


# Ensure both train and validation sets have both classes
# You might need to adjust the 'stratify' parameter based on your data
if len(train_df) > 0 and len(val_df) > 0:
  train_df, val_df = train_test_split(
      pd.concat([train_df, val_df]),  # Combine and then split
      test_size=0.2,  # Adjust the test_size as needed
      random_state=42,  # Set a random seed for reproducibility
      stratify=pd.concat([train_df, val_df])['class']  # Ensure class distribution
  )
elif len(train_df) == 0:
  print ("Error: train_df is empty")
elif len(val_df) == 0:
  print ("Error: val_df is empty")
  train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['class'])

def validate_filenames(df, folder):
    valid_files = [filename for filename in df['filename'] if os.path.exists(os.path.join(folder, filename))]
    return valid_files

train_df['valid'] = train_df['filename'].apply(lambda x: os.path.exists(os.path.join(train_folder, x)))
val_df['valid'] = val_df['filename'].apply(lambda x: os.path.exists(os.path.join(val_folder, x)))

print("Valid training filenames:", train_df[train_df['valid']].shape[0])
print("Valid validation filenames:", val_df[val_df['valid']].shape[0])

train_df = train_df[train_df['valid']].drop(columns=['valid'])
val_df = val_df[val_df['valid']].drop(columns=['valid'])


# Initialize ImageDataGenerator
train_datagen = ImageDataGenerator(rescale=1./255,
                                    rotation_range=20,
                                    width_shift_range=0.2,
                                    height_shift_range=0.2,
                                    shear_range=0.2,
                                    zoom_range=0.2,
                                    horizontal_flip=True)

val_datagen = ImageDataGenerator(rescale=1./255)

label_encoder = LabelEncoder()

# Fit the encoder on all unique labels from both train and validation sets
all_labels = pd.concat([train_df['class'], val_df['class']]).unique()
label_encoder.fit(all_labels)

# Transform labels in your DataFrames
#train_df['class'] = label_encoder.transform(train_df['class'])
#val_df['class'] = label_encoder.transform(val_df['class'])


# Create flow generators from the DataFrames
train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_df,
    directory=train_folder,  # Ensure correct directory
    x_col='filename',
    y_col='class',
    target_size=(224, 224),
    class_mode='binary',
    batch_size=32
)

if not val_df.empty:
  val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_df,
    directory=val_folder,
    x_col='filename',
    y_col='class',
    target_size=(224, 224),
    class_mode='binary',
    batch_size=32
)
  print("Validation generator created.")
else:
  print("Validation DataFrame is empty. Skipping validation generator creation.")

print("Validation Generator Summary:")
print(f"Total Validation Samples: {val_generator.samples}")
print(f"Class Indices: {val_generator.class_indices}")
print(len(val_generator))

for batch in train_generator:
    x, y = batch
    print(f'Batch shape: {x.shape}, Labels: {y}')
    break


# Define the model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # For binary classification
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
if train_df.empty or val_df.empty:
    print("Error: train_df or val_df is empty.")
else:
  history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    validation_data=val_generator,
    validation_steps=2,
    epochs=10,
    callbacks=[early_stopping]
)

# Evaluate the model on the validation set
val_loss, val_accuracy = model.evaluate(val_generator, steps=len(val_generator))
print(f"Validation Accuracy: {val_accuracy * 100:.2f}%")

!pip install -U pyngrok
from pyngrok import ngrok
import tensorflow as tf
from tensorflow.keras.preprocessing import image
import numpy as np
from flask import Flask, request, render_template_string
import os

ngrok.set_auth_token("2o5PhDblHZJyMFrb2sA5wEDLdH2_Yi5UMpDK1gkRrFodwx8U")

# Load the saved model
model = tf.keras.models.load_model("coin_model.keras")

# Prediction function
def predict_coin(image_path):
    img = image.load_img(image_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Normalize

    prediction = model.predict(img_array)
    return "Double Die" if prediction[0][0] > 0.4 else "Single Die"

# Flask app setup
app = Flask(__name__)

# HTML templates as strings
upload_template = """
<!doctype html>
<html lang="en">
<head>
  <title>Coin Error Detection</title>
</head>
<body>
  <h1>Upload a Coin Image</h1>
  <form method="post" enctype="multipart/form-data">
    <input type="file" name="file">
    <button type="submit">Predict</button>
  </form>
</body>
</html>
"""

result_template = """
<!doctype html>
<html lang="en">
<head>
  <title>Prediction Result</title>
</head>
<body>
  <h1>Prediction: {{ prediction }}</h1>
  <a href="/">Upload another image</a>
</body>
</html>
"""

# Flask route to handle uploads and predictions
@app.route("/", methods=["GET", "POST"])
def upload_file():
    if request.method == "POST":
        file = request.files["file"]
        if file:
            file_path = "uploaded_image.jpg"
            file.save(file_path)
            result = predict_coin(file_path)
            os.remove(file_path)
            return render_template_string(result_template, prediction=result)
    return render_template_string(upload_template)

# Start the Flask app
port = 5000
public_url = ngrok.connect(5000).public_url # Expose the app
print(" * ngrok URL:", public_url)
app.run()
